{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.8"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Adder using RNN in Keras\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-67c8b658dcd2>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-67c8b658dcd2>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    <a href=\"https://colab.research.google.com/github/luckykadam/adder/blob/master/rnn_full_adder.ipynb\">\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "<a href=\"https://colab.research.google.com/github/luckykadam/adder/blob/master/rnn_full_adder.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab\n",
    "</a>\n",
    "<a href=\"https://github.com/luckykadam/adder/blob/master/rnn_full_adder.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Introduction\n",
    "It's rare to encounter a situation where LSTM/GRU might not be the choice of RNN cell. How hard it would be to identify and work-around the problem? Read on to find out.\n",
    "\n",
    "In this notebook, we will emulate Binary Full Adder using RNN in Keras. We will see:\n",
    "\n",
    "1. why at some situations LSTM/GRU might not be the most optimal choice\n",
    "2. how to write custom RNN layer"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Background\n",
    "\n",
    "In the <a href=\"https://github.com/luckykadam/adder/blob/master/full_adder.ipynb\">previous post</a> we developed a small neural network to simulate binary full adder. We analysed all the parameters learnt, plotted decision hypersurfaces and drew the circuit. Later, we observed how much the usage pattern resembled Recurrent Neural Network. So, lets see how to achieve the same objective using RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Full Adder\n",
    "\n",
    "A Full Adder can perform an addition operation on three bits. The full adder produces a sum of three inputs and carry value. The carry value can then be used as input to the next full adder.\n",
    "\n",
    "Using this unit in repeatition, two binary numbers of arbitrary length can be added.\n",
    "\n",
    "<img height=\"220\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/6/69/Full-adder_logic_diagram.svg/800px-Full-adder_logic_diagram.svg.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RNN eumlation\n",
    "\n",
    "The structure of Full Adder is very similar to <a href=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/\">how RNN works</a>. We can expolit this similarity.\n",
    "\n",
    "In the current context, we want RNN cell with output and state, both of dimension 1, but representing independent information. Let's have a look at common choices of RNN cells:\n",
    "\n",
    "1. **LSTM** (Long-Short Term Memory): //TODO: add reason to reject LSTM\n",
    "2. **GRU** (Gated Recurrent Unit): This cell has output and state of same size, but they are not independent. Hence, not suitable here.\n",
    "\n",
    "So, I guess we will have to define out own custom RNN cell. Lets jump right in ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementation\n",
    "\n",
    "We are going to use Keras (`tf.keras` from Tensorflow 2.0) and it's `keras.layers.RNN` API to implement out RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for Google Colab compatibiity\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "2.0.0\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, activations\n",
    "\n",
    "\n",
    "print(tf.__version__)\n",
    "# set random seed to get reproducible results\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset creation\n",
    "\n",
    "Dataset can be easily prepared by randomly generating two sets of numbers and adding these sets across to get expected result. We generate numbers with a limit to number of bits in binary representation: `max_bit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_bits = 8\n",
    "n_samples = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples in decimal form\n",
    "samples = np.random.randint(np.power(2, max_bits-1), size=(n_samples, 2))\n",
    "summed_samples = np.sum(samples, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert samples to binary representation\n",
    "samples_binary_repr = [[np.binary_repr(a, width=max_bits), np.binary_repr(b, width=max_bits)] for a,b in samples]\n",
    "summed_binary_repr = [np.binary_repr(c, width=max_bits) for c in summed_samples]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_str = np.array([[list(a), list(b)] for a, b in samples_binary_repr])\n",
    "y_str = np.array([list(c) for c in summed_binary_repr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flip binary representation to get increasing significant bit\n",
    "x_flipped = np.flip(x_str, axis=-1)\n",
    "y_flipped = np.flip(y_str, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert string to numbers\n",
    "x = np.transpose((x_flipped == '1')*1, axes=(0, 2, 1))\n",
    "y = (y_flipped == '1')*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RNN Cell\n",
    "\n",
    "We are going to create an RNN cell: with three inputs (i<sup>th</sup> bit of the 2 numbers and previous carry), one hidden layer (3 neurons) and one output layer (2 neurons). Out of two output bits, we want one to be a part of the answer and other to be input (carry) to the next RNN cell.\n",
    "\n",
    "We extend `keras.layers.Layer` to define the custom RNN cell. To define any custom layer we need to follow these steps:\n",
    "\n",
    "1. define `__init__()` to initialize some object level constants. Keras requires you to declare `units` variable: dimension of the output.\n",
    "2. define `build()` to initialize all the trainable parameters and set `built=True`.\n",
    "3. define `call()` to compute the output (and state) using input and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullAdderCell(layers.Layer):\n",
    "    def __init__(self, hidden_units, **kwargs):\n",
    "        super(FullAdderCell, self).__init__(**kwargs)\n",
    "        self.units = 1\n",
    "        self.state_size = 1\n",
    "        self.hidden_units = hidden_units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.hidden_kernel = self.add_weight(shape=(input_shape[-1] + self.state_size, self.hidden_units),\n",
    "                                      initializer='uniform',\n",
    "                                      name='hidden_kernel')\n",
    "        self.hidden_bias = self.add_weight(shape=(1, self.hidden_units),\n",
    "                                      initializer='uniform',\n",
    "                                      name='hidden_bias')\n",
    "        self.output_kernel = self.add_weight(shape=(self.hidden_units, self.units + self.state_size),\n",
    "                                      initializer='uniform',\n",
    "                                      name='output_kernel')\n",
    "        self.output_bias = self.add_weight(shape=(1, self.units + self.state_size),\n",
    "                                      initializer='uniform',\n",
    "                                      name='output_bias')\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, states):\n",
    "        x = tf.concat([inputs, states[0]], axis=-1)\n",
    "        h = tf.keras.activations.tanh(tf.matmul(x, self.hidden_kernel) + self.hidden_bias)\n",
    "        o_s = tf.keras.activations.sigmoid(tf.matmul(h, self.output_kernel) + self.output_bias)\n",
    "        output = o_s[:, :self.units]\n",
    "        state = o_s[:, self.units:]\n",
    "        return output, [state]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model\n",
    "\n",
    "`Sequential` API can be used to define the model. We need to wrap the RNN cell with `keras.layer.RNN`, to get an RNN layer. We set `return_sequences=True`, because we want to collect the bits produced by RNN cell at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"full_adder\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nrnn (RNN)                    (None, None, 1)           20        \n=================================================================\nTotal params: 20\nTrainable params: 20\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model = tf.keras.Sequential(name='full_adder')\n",
    "model.add(layers.RNN(FullAdderCell(3), return_sequences=True, input_shape=(None, 2)))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loss function\n",
    "\n",
    "At each step, only one bit is produced, giving the output of shape `(batch_size, max_bits, 1)`, hence we use `binary_crossentropy` loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Train on 90000 samples\nEpoch 1/5\n90000/90000 [==============================] - 14s 151us/sample - loss: 0.6916 - accuracy: 0.5087\nEpoch 2/5\n90000/90000 [==============================] - 12s 138us/sample - loss: 0.3403 - accuracy: 0.8986\nEpoch 3/5\n90000/90000 [==============================] - 13s 147us/sample - loss: 0.0644 - accuracy: 1.0000\nEpoch 4/5\n90000/90000 [==============================] - 13s 144us/sample - loss: 0.0131 - accuracy: 1.0000\nEpoch 5/5\n90000/90000 [==============================] - 14s 154us/sample - loss: 0.0034 - accuracy: 1.0000\n10000/1 - 1s - loss: 0.0017 - accuracy: 1.0000\n"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=32, epochs=5)\n",
    "scores = model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing\n",
    "\n",
    "Let's generate two random numbers in range (0, 2<sup>max_bits-1</sup>), predict their sum using our network, and compare it with actual sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "a: 48, b: 10\nbinary representations -> a: [0. 0. 0. 0. 1. 1. 0. 0.], b: [0. 1. 0. 1. 0. 0. 0. 0.]\na_b: [[[0. 0.]\n  [0. 1.]\n  [0. 0.]\n  [0. 1.]\n  [1. 0.]\n  [1. 0.]\n  [0. 0.]\n  [0. 0.]]]\n"
    }
   ],
   "source": [
    "max_bits = 8\n",
    "\n",
    "a = np.random.randint(np.power(2, max_bits-1))\n",
    "b = np.random.randint(np.power(2, max_bits-1))\n",
    "\n",
    "a_bin = np.float32(1) * (np.flip(list(np.binary_repr(a, width=max_bits)), axis=-1) == '1')\n",
    "b_bin = np.float32(1) * (np.flip(list(np.binary_repr(b, width=max_bits)), axis=-1) == '1')\n",
    "\n",
    "print('a: {}, b: {}'.format(a, b))\n",
    "print('binary representations -> a: {}, b: {}'.format(a_bin, b_bin))\n",
    "\n",
    "a_b = np.stack((a_bin, b_bin), axis=-1).reshape(1,-1,2)\n",
    "print('a_b: {}'.format(a_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "predictions: [0.00120685 0.998754   0.00120774 0.998754   0.9987539  0.9987539\n 0.00120774 0.00120688]\nbinary representations -> summed: [0 1 0 1 1 1 0 0]\nsummed: 58\n"
    }
   ],
   "source": [
    "predictions = model(a_b).numpy().flatten()\n",
    "\n",
    "summed_bin = 1 * (predictions > 0.5)\n",
    "summed = np.packbits(np.flip(summed_bin , axis=-1))[0]\n",
    "print('predictions: {}'.format(predictions))\n",
    "print('binary representations -> summed: {}'.format(summed_bin))\n",
    "print('summed: {}'.format(summed))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Result\n",
    "\n",
    "Voila! Our network worked perfectly. Its amazing, how easily we created a custom RNN layer using `keras.layer.RNN` and `keras.layer.Layer` APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Conclusion\n",
    "\n",
    "Frameworks like Keras, Tensorflow and PyTorch give us power to experiment at such speed and effeciecy. Combine it with python's flexibility, and you have got a game-changer in AI department."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## References:\n",
    "\n",
    "1. https://en.wikibooks.org/wiki/Digital_Electronics/Digital_Adder\n",
    "2. https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "3. https://www.tensorflow.org/guide/keras/rnn"
   ]
  }
 ]
}