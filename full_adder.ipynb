{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.8"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Full Adder in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Background\n",
    "\n",
    "Neurons (with non-linear activation) are only capable of learning a linear decision boundary, but when sufficient units and layers of neurons work together, they can learn theoretically any complex function. This property of NNs has been mathematically proven, and demonstrated via many examples.\n",
    "\n",
    "To understand the fundamentals of how can neural networks achieve this, it is more beneficial for a new-comer to analyse the individual parameters, and how they contribute to the final output, instead of starting with complex task like cat/dog image classification, which might be overwhelming.\n",
    "\n",
    "Today, we will build a very simple neural network, to simlulate the binary full adder, and analyse its parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Binary Adder\n",
    "\n",
    "Binary Adder is a circuit to add two binary numbers.\n",
    "\n",
    "### Half Adder\n",
    "\n",
    "The Half Adder is used to add two binary bits. The half adder outputs a sum of the two inputs and a carry value.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d9/Half_Adder.svg/360px-Half_Adder.svg.png\">\n",
    "\n",
    "### Full Adder\n",
    "\n",
    "A Full Adder can perform an addition operation on three bits. The full adder produces a sum of the three inputs and carry value. The carry value can then be used as one of the inputs to the next full adder.\n",
    "\n",
    "Using this unit in repeatition, two binary numbers of arbitrary length can be added.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/6/69/Full-adder_logic_diagram.svg/800px-Full-adder_logic_diagram.svg.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logic Gates\n",
    "\n",
    "Some logic gates which have a linear decision boundary(eg. AND, OR, NAND, NOR) can be represented using a single neuron, where as some logic gates don't have a linear decision boundary(eg. XOR), require multiple neurons or layers.\n",
    "\n",
    "TODO: clarify the number of neurons required for different types of logic gates\n",
    "<p>\n",
    "TODO: insert images to show decision boundary of different gates"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementation\n",
    "\n",
    "We are going to use Keras (from Tensorflow 2) to create neural network that can simulate full adder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, activations\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset creation\n",
    "\n",
    "There are only 8 possible combination to inputs and ouputs. To create the dataset, we are going to repeat these unique samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100000//8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0, 0, 0], [0, 1, 0], [1, 0, 0], [1, 1, 0], [0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]] * n_samples) # a, b, c\n",
    "y = np.array([[0, 0], [0, 1], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [1, 1]] * n_samples) # c, s"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk-learn provides great functionality to split the dataset into train and test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model\n",
    "\n",
    "We model the problem as classification of sum bit as active/inactive and carry bit as active/inactive.\n",
    "\n",
    "Keras provides multiple ways to define a model.\n",
    "\n",
    "1. `Sequential` API provides an easy interface to create a model, where data flows sequentially through a stack of layers.\n",
    "2. `Model` Functional API can define complex models, such as multi-output models, directed acyclic graphs, or models with shared layers.\n",
    "3. To get even more control, Keras allows to extend `Model` class and override `__init__` and `call` functions.\n",
    "\n",
    "In our example we don't want any feedback loop, hence `Sequencial` model would work. The layers in out network are:\n",
    "\n",
    "1. `Input`: Inputs to the network are 3 bits: a, b and c (carry).\n",
    "2. `Dense`: TODO: justify dimension of hidden layer\n",
    "3. `Dense`: Outputs are 2 bits: c (next carry), s (sum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"full_adder\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nhidden (Dense)               (None, 3)                 12        \n_________________________________________________________________\noutput (Dense)               (None, 2)                 8         \n=================================================================\nTotal params: 20\nTrainable params: 20\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model = models.Sequential(name='full_adder')\n",
    "model.add(layers.Input(shape=(3,), name='input'))\n",
    "model.add(layers.Dense(units=3, activation=activations.tanh, name='hidden'))\n",
    "model.add(layers.Dense(units=2, activation=activations.sigmoid, name='output'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loss function\n",
    "\n",
    "Choice of loss function is a crucial step. The most common choices for different types of problems are:\n",
    "\n",
    "1. **Regression**: Default choice is `mean_squared_error`.\n",
    "2. **Classification**:\n",
    "  * **Binary Classification**: Default choice is `binary_crossentropy`. The function requires that the output layer is configured with a `sigmoid` activation to limit output in (0, 1).\n",
    "  * **Multi-Class Single-Label Classification**: When a sample can belong to any one class (of >2 options), default choice is `categorical_crossentropy`. This also requires a `sigmoid` activated output layer.\n",
    "  * **Multi-Class Multi-Label Classification**: When a sample might belong to multiple classes (of >1 options), default choice is `binary_crossentropy`. Here each classification is considered independent of each other, and effectively multiple binary classifiers are learned at once. This also requires a `sigmoid` activated output layer.\n",
    "\n",
    "In out Full Adder model both the outputs can be '1' at the same time, hence it would be a multi-class multi-label classification.\n",
    "\n",
    "On choosing optimizer, `adam` works well in most cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training\n",
    "\n",
    "When training set is large, each iteration (parameter update step) might take very long. This way progress in parameters is very slow.\n",
    "\n",
    "One trick to fasten this process is to consider small subset of train set, that fairly represents the complete train set, for each iteration. That way iterations would complete in short durations, and parameters is would be updated more frequently.\n",
    "\n",
    "How would we create those small subset? Simplest way is to shuffle the train set and select subsets sequentially.\n",
    "\n",
    "This trick is called **Mini Batching**. In keras default method is mini batching controlled by `mini_batch` parameter of `Model.fit` method. To disable set `batch_size` equal to train set size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Train on 90000 samples\nEpoch 1/3\n90000/90000 [==============================] - 4s 46us/sample - loss: 0.4455 - accuracy: 0.8045\nEpoch 2/3\n90000/90000 [==============================] - 3s 34us/sample - loss: 0.1598 - accuracy: 0.9767\nEpoch 3/3\n90000/90000 [==============================] - 3s 36us/sample - loss: 0.0315 - accuracy: 1.0000\n10000/1 - 0s - loss: 0.0154 - accuracy: 1.0000\n[0.01452168820798397, 1.0]\n"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=32, epochs=3)\n",
    "scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add arbitrary length numbers using this network"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-16-6389a7a9cd74>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-6389a7a9cd74>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    1. https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## References:\n",
    "\n",
    "1. https://en.wikibooks.org/wiki/Digital_Electronics/Digital_Adder\n",
    "2. https://towardsdatascience.com/neural-representation-of-logic-gates-df044ec922bc\n",
    "3. https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/"
   ]
  }
 ]
}