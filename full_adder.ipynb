{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.8"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Full Adder in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/luckykadam/adder/blob/master/full_adder.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/luckykadam/adder/blob/master/full_adder.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Background\n",
    "\n",
    "Neurons (with non-linear activation) are only capable of learning a linear decision boundary, but when sufficient units and layers of neurons work together, they can learn theoretically any complex function. This property of NNs has been mathematically proven, and demonstrated via many examples.\n",
    "\n",
    "To understand the fundamentals of how can neural networks achieve this, it is more beneficial for a new-comer to analyse the individual parameters, and how they contribute to the final output, instead of starting with complex task like cat/dog image classification, which might be overwhelming.\n",
    "\n",
    "Today, we will build a very simple neural network, to simlulate the binary full adder and analyse its parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Binary Adder\n",
    "\n",
    "Binary Adder is a circuit to add two binary numbers.\n",
    "\n",
    "### Half Adder\n",
    "\n",
    "The Half Adder is used to add two binary bits. The half adder outputs the sum of two input bits and a carry value.\n",
    "\n",
    "<img height=\"100\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d9/Half_Adder.svg/360px-Half_Adder.svg.png\">\n",
    "\n",
    "### Full Adder\n",
    "\n",
    "A Full Adder can perform an addition operation on three bits. The full adder produces a sum of three inputs and carry value. The carry value can then be used as input to the next full adder.\n",
    "\n",
    "Using this unit in repeatition, two binary numbers of arbitrary length can be added.\n",
    "\n",
    "<img height=\"180\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/6/69/Full-adder_logic_diagram.svg/800px-Full-adder_logic_diagram.svg.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logic Gates\n",
    "\n",
    "Some logic gates which have a linear decision boundary(eg. AND, OR, NAND, NOR) can be represented using a single neuron, whereas some logic gates don't have a linear decision boundary(eg. XOR) and require multiple neurons or layers.\n",
    "\n",
    "TODO: clarify the number of neurons required for different types of logic gates\n",
    "<p>\n",
    "TODO: insert images to show decision boundary of different gates"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementation\n",
    "\n",
    "We are going to use Keras (`tf.keras`) to create a neural network that can simulate full adder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, activations\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset creation\n",
    "\n",
    "There are only 8 possible combination to inputs and ouputs. To create the dataset, we are going to repeat these unique samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100000//8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0, 0, 0], [0, 1, 0], [1, 0, 0], [1, 1, 0], [0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]] * n_samples) # a, b, c\n",
    "y = np.array([[0, 0], [0, 1], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [1, 1]] * n_samples) # c, s"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk-learn provides great functionality to split the dataset into train and test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model\n",
    "\n",
    "We model the problem as classification of sum bit as active/inactive and carry bit as active/inactive.\n",
    "\n",
    "Keras provides multiple ways to define a model.\n",
    "\n",
    "1. `Sequential` API provides an easy interface to create a model, where data flows sequentially through a stack of layers.\n",
    "2. `Model` Functional API can define complex models, such as multi-output models, directed acyclic graphs, or models with shared layers.\n",
    "3. To get even more control, Keras allows to extend `Model` class and override `__init__` and `call` functions.\n",
    "\n",
    "In our example we don't want any feedback loop, hence `Sequencial` model would work. The layers in out network are:\n",
    "\n",
    "1. `Input`: Inputs to the network are 3 bits: a, b and c (carry).\n",
    "2. `Dense`: TODO: justify dimension of hidden layer\n",
    "3. `Dense`: Outputs are 2 bits: c (next carry), s (sum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"full_adder\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nhidden (Dense)               (None, 3)                 12        \n_________________________________________________________________\noutput (Dense)               (None, 2)                 8         \n=================================================================\nTotal params: 20\nTrainable params: 20\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model = models.Sequential(name='full_adder')\n",
    "model.add(layers.Input(shape=(3,), name='input'))\n",
    "model.add(layers.Dense(units=3, activation=activations.tanh, name='hidden'))\n",
    "model.add(layers.Dense(units=2, activation=activations.sigmoid, name='output'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loss function\n",
    "\n",
    "Choice of loss function is a crucial step. The most common choices for different types of problems are:\n",
    "\n",
    "1. **Regression**: Default choice is `mean_squared_error`.\n",
    "2. **Classification**:\n",
    "  * **Binary Classification**: Default choice is `binary_crossentropy`. The function requires that the output layer is configured with a `sigmoid` activation to limit output to (0, 1).\n",
    "  * **Multi-Class Single-Label Classification**: When a sample can belong to any one class (of >2 options), default choice is `categorical_crossentropy`. This also requires a `sigmoid` activated output layer.\n",
    "  * **Multi-Class Multi-Label Classification**: When a sample might belong to multiple classes (of >1 options), default choice is `binary_crossentropy`. Here each classification is considered independent of each other, and effectively multiple binary classifiers are learned at once. This also requires a `sigmoid` activated output layer.\n",
    "\n",
    "In out Full Adder model both the outputs can be '1' at the same time, hence it would be a multi-class multi-label classification.\n",
    "\n",
    "On choosing optimizer, `adam` works well in most cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training\n",
    "\n",
    "When training set is large, each iteration (parameter update step) might take very long. This way progress in parameters is very slow.\n",
    "\n",
    "One trick to fasten this process is to consider small subset of train set, that fairly represents the complete train set, for each iteration. That way iterations would complete in short durations, and parameters is would be updated more frequently.\n",
    "\n",
    "How would we create those small subset? Simplest way is to shuffle the train set and select subsets sequentially.\n",
    "\n",
    "This trick is called **Mini-Batching**. In keras default method is mini batching controlled by `mini_batch` parameter of `Model.fit()` method. To disable mini-batching, set `batch_size` parameter to size of train set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Train on 90000 samples\nEpoch 1/3\n90000/90000 [==============================] - 4s 44us/sample - loss: 0.4455 - accuracy: 0.8045\nEpoch 2/3\n90000/90000 [==============================] - 4s 41us/sample - loss: 0.1598 - accuracy: 0.9767\nEpoch 3/3\n90000/90000 [==============================] - 3s 32us/sample - loss: 0.0315 - accuracy: 1.0000\n10000/1 - 0s - loss: 0.0154 - accuracy: 1.0000\n[0.01452168820798397, 1.0]\n"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=32, epochs=3)\n",
    "scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing\n",
    "\n",
    "Let's write some code to add two numbers of arbitraty length.\n",
    "\n",
    "Generate two random numbers in range (0, 2<sup>`max_bits`-1</sup>), so that total sum is in range (0, 2<sup>`max_bits`</sup>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "a: 18, b: 39\nbinary representations -> a: [0 1 0 0 1 0 0 0], b: [1 1 1 0 0 1 0 0]\n"
    }
   ],
   "source": [
    "max_bits = 8\n",
    "\n",
    "a = np.random.randint(np.power(2, max_bits-1))\n",
    "b = np.random.randint(np.power(2, max_bits-1))\n",
    "\n",
    "a_bin = 1 * (np.flip(list(np.binary_repr(a, width=max_bits)), axis=-1) == '1')\n",
    "b_bin = 1 * (np.flip(list(np.binary_repr(b, width=max_bits)), axis=-1) == '1')\n",
    "\n",
    "print('a: {}, b: {}'.format(a, b))\n",
    "print('binary representations -> a: {}, b: {}'.format(a_bin, b_bin))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-feb70e14793f>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-feb70e14793f>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Now, iterate over bits of both numbers,\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Now, iterate over bits of both numbers, feeding them with carry bit to the model. The ouput at each step is used to evaluate the corresponding bit of summed value, and update the carry bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "predictions: [0.98617655 0.03193694 0.01723741 0.96571255 0.97988051 0.98609447\n 0.03419748 0.03421632]\nbinary representations -> summed: [1 0 0 1 1 1 0 0]\nsummed: 57\n"
    }
   ],
   "source": [
    "c = 0\n",
    "predictions = np.zeros(max_bits)\n",
    "\n",
    "for i in range(max_bits):\n",
    "    a_b_c = np.array([[a_bin[i], b_bin[i], c]], dtype='float32')\n",
    "    c_s = model(a_b_c)[0]\n",
    "    c = c_s[0]\n",
    "    predictions[i] = c_s[1]\n",
    "\n",
    "print('predictions: {}'.format(predictions))\n",
    "\n",
    "summed_bin = 1 * (predictions > 0.5)\n",
    "print('binary representations -> summed: {}'.format(summed_bin))\n",
    "\n",
    "summed = np.packbits(np.flip(summed_bin , axis=-1))[0]\n",
    "print('summed: {}'.format(summed))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Result\n",
    "\n",
    "As we can see, our model works perfectly."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Further Reading\n",
    "\n",
    "Similar implementation of half adder can be found at: [github.com/luckykadam/adder/blob/master/half_adder.ipynb]"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-16-6389a7a9cd74>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-6389a7a9cd74>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    1. https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## References:\n",
    "\n",
    "1. https://en.wikibooks.org/wiki/Digital_Electronics/Digital_Adder\n",
    "2. https://towardsdatascience.com/neural-representation-of-logic-gates-df044ec922bc\n",
    "3. https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/"
   ]
  }
 ]
}